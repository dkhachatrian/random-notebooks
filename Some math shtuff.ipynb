{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some math shtuff\n",
    "\n",
    "A quick(ish) reference for some concepts whose intuitions/simple methods of understanding sometimes escape me.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy, Cross-Entropy, KL Divergence\n",
    "\n",
    "There are a number of ways to think about/get to the Shannon entropy ($H$) (thankfully not $S$...) One simple way is to say what we want it to mean qualitatively and impose some desired characteristics to determine its mathematical formulation. In this case, it may make more sense to start with cross-entropy first.\n",
    "\n",
    "Say you're sending me messages from an alphabet of symbols $A$. Some are more likely than others, and we'll call the probability distribution associated with the actual generator of symbols $p$. I'm over here on the other side thinking like I'm a pair of smartypants ~~and start looking to spread my brand through nationwide department stores~~ that has figured out how likely you are to send each symbol to me. We'll call my expected distribution (which may or may not be the correct distribution) $q$.\n",
    "\n",
    "Now we want to measure how surprised I am by any given message you send me -- let's call it $\\tau$ (rhymes with \"wow\" -- alas, $\\tau$ is not standard notation). We would imagine the following properties would be useful for $\\tau$ to have:\n",
    "1. The more surprised I am, the larger $\\tau$ gets (otherwise, it wouldn't exactly be doing a good job measuring my surprise)\n",
    "2. If symbols are independently generated, we can construct the total surprise of my message by adding the total surprise of each symbol in the message, i.e., $\\tau_{M} = \\sum_{i=1}^{m} \\tau_{M[i]}$ where $M \\in A^m$ is a string of symbols, $m$ is the cardinality of $M$, and $M[i]$ denotes the $i$'th element of $M$.\n",
    "\n",
    "Using these two criteria, a reasonable mapping is\n",
    "\n",
    "$$\\tau_{a} = \\log\\left(\\frac{1}{q_a}\\right), a \\in A$$\n",
    "\n",
    "where $q_a$ is the probability I think you'll send the symbol $a$.\n",
    "\n",
    "I mean, that's great and all, but that works for individual instances of strings or symbols. How much should I *expect* to be surprised by any given symbol? Well, that'd depend on how often you *actually* send me a symbol, alongside how often I expect to receive that particular symbol.\n",
    "\n",
    "Hmm, this smells of expectation! And indeed, that's all we do -- take the expectation over $A$ (using the *actual* probability of each symbol occurring, i.e., using $p$) of my surprise per symbol:\n",
    "\n",
    "$$E_{a \\sim p}[\\tau] = \\sum_{a \\in A} p_a \\log\\left(\\frac{1}{q_a}\\right) = H(p,q)$$\n",
    "\n",
    "We denote this metric $H(p,q)$ (where $p$ and $q$ are the actual and presumed distributions, respectively) and call it the **cross-entropy** (because that sounds pretty cyberpunk to me. I like to think they throw in \"cross\" because it measures the surprise caused by \"crossing\" the distributions $p$ and $q$ together.)\n",
    "\n",
    "Now, we'd imagine that I'd be the least surprised if my presumed distribution of symbols were in fact the actual distribution, i.e. when $p = q$. In fact, this is true!\n",
    "\n",
    "$$H(p,p) = H(p) = \\sum_{a \\in A} p_a \\log\\left(\\frac{1}{p_a}\\right)$$\n",
    "\n",
    "is called the **entropy** (or **Shannon entropy**) of the probability distribution and written $H$ (thankfully not $S$...). Usually, the $log$s written above are base-2. This permits a way of thinking of the value of the Shannon entropy: if I'm only allowed to ask the same series of questions to you to figure out which symbol you want to send me and I know the actual probability distribution $p$ of symbols, how many questions should I expect to ask (i.e. mean/average) before I figure out the answer? The cross-entropy is the same thing, expect I don't necessarily know the actual probability distribution $p$ of symbols, I just think I do (and I think it's $q$) and base my series of questions based on $q$.\n",
    "\n",
    "Now, hopefully that makes it clear that $ p \\neq q \\implies H(p,q) > H(p) $ -- if I don't know the actual distribution, I'm not going to be able to answer the most optimal series of questions. This suggestions to us a notion of \"distance\" (i.e. a metric) between the probability distributions $p$ and $q$:\n",
    "\n",
    "$$ KL(p \\mid\\mid q) = H(p,q) - H(p) $$\n",
    "\n",
    "This metric is called the **Kullbackâ€“Leibler divergence** (because names) or the **KL divergence** (because initialisms), or seemingly most rarely but probably most clearly the **relative entropy**. Out loud, you'd say $KL(p \\mid\\mid q)$ is \"the \\[blah\\] of p with respect to q\". The closer the KL divergence is to 0, the closer $q$ is to being $p$, and $ KL(p \\mid\\mid q) = 0 \\implies p = q $ at every point in the domain of $q$ (which is the same as the domain of $p$).\n",
    "\n",
    "This makes describing the **mutual information** of random variables X and Y in terms of a KL divergence fairly intuitive. Just running off the name, if X and Y were independent, you'd expect no mutual information between them. In such a case, $P(X,Y) = P(X)P(Y)$, and $ KL\\left(P(X,Y)\\mid\\mid P(X)P(Y)\\right) = 0$. And in fact, one way of writing the mutual information of random variables X and Y is exactly\n",
    "\n",
    "$$ I(X;Y) = KL\\left(P(X,Y)\\mid\\mid P(X)P(Y)\\right) $$\n",
    "\n",
    "which I think is pretty neat.\n",
    "\n",
    "\\- DK (4/24/18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "I can never seem to remember Bayes' Theorem directly, as they write it out in textbooks. It makes so much more sense to me to think about it from the relationships between conditional and joint probabilites/distributions, and one of the common tricks to make Bayes' Theorem useful in practice also comes to me far more easily when explicitly thinking about events as being sampled from a *sample space* of possibilities/outcomes.\n",
    "\n",
    "\n",
    "\n",
    "Consider two possible events A and B. Let's keep in mind that A is just one possible outcome out of a set of possibilities, as is B; we'll say $\\alpha$ is the set of possibilities from which $A$ was drawn and $\\beta$ is the set of possibilities from which $B$ was drawn, i.e. $A \\sim \\alpha$ and $B \\sim \\beta$ (this will be good to remember later). Now:\n",
    "\n",
    "$$ \\text{Pr[A and B both occur]} := P(A,B) $$\n",
    "\n",
    "Assuming individual events happen separately, there are two ways for both A and B to occur:\n",
    "1. A happens first, then B happens.\n",
    "2. B happens first, then A happens.\n",
    "\n",
    "(\"Duh\", I know.)\n",
    "\n",
    "Keeping in mind that the first event might affect the probability of the second event occurring (i.e. remembering that conditional probabilities exist), we can write:\n",
    "\n",
    "$$ P(A,B) = P(A) \\times P(B \\mid A) = P(B) \\times P(A \\mid B) $$\n",
    "\n",
    "And then it's simple to write out Bayes' Theorem as it's often written (we'll write it perhaps a bit more evocatively):\n",
    "\n",
    "$$ P(B) \\times P(A \\mid B) = P(A) \\times P(B \\mid A) $$\n",
    "\n",
    "$$\\begin{align} P(A \\mid B) &= \\frac{P(A) \\times P(B \\mid A)} {P(B)} \\\\\n",
    "                        &= P(A) \\times \\frac {P(B \\mid A)} {P(B)} \\end{align}$$\n",
    "\n",
    "Using the Bayesian interpretation: At first we thought the probability that $A$ occurs is $P(A)$. After we saw that $B$ happened, we re-evaluate the probability that $A$ occurs with a scaling factor $ \\frac {P(B \\mid A)} {P(B)} $\n",
    "\n",
    "Also worth knowing the fancy terminology:\n",
    "- the **_a priori_ probability** or just the **prior** is what we thought would be the probability that a random variable takes on a certain value before we observed anything. So the *a priori* probability (or just prior) for the event $A$ would be $P(A)$. If we consider $A$ to be a random variable instead of an event, we're guessing the distribution of $A$ and so $P(A)$ would be an **_a priori_ distribution** (or again, just the prior).\n",
    "- the **_a posteriori_ probability** or just the **posterior** is what we think the probability that a random variable takes on a certain value is after observing something. In this case, the *a posteriori* probability (or just posterior) of the event $A$ after observing $B$ is $P(A \\mid B)$. If we consider $A$ to be a random variable instead of an event, we're guessing the distribution of $A$ after observing a random variable/event B and so $P(A \\mid B)$ would be an **_a posteriori_ distribution**, (or again, just the posterior).\n",
    "\n",
    "\n",
    "Now, in cases of inference, we normally have some data on the likelihood of one of the conditionals -- let's say $P(B \\mid A)$ to avoid flipping our equation around again -- via empirical counts. So we've already guessed some prior $P(A)$, and we're trying to improve it by calculating the posterior $ P(A \\mid B) $. But what if we don't know $ P(B) $? Is our guessing and data collection all for naught!? Thankfully, not so! The answer lies right under our noses -- or in this case, in our previous calculations.\n",
    "\n",
    "Consider $P(A,B)$ again. What would we get if we added $P(A,B)$ over all possible values of A? (Remember we said that $A \\sim \\alpha$, so A could have been some other event within the set $\\alpha$.) That's basically just saying that we don't care what value $A$ is, so we end up with $P(B)$! ('!' used to denote excitement, not factorialization.) And conveniently, we'd already have a way to estimate these values:\n",
    "\n",
    "$$\\begin{align} P(B) &= \\sum_{A \\in \\alpha} P(A,B) \\\\\n",
    "                     &= \\sum_{A \\in \\alpha} P(A) \\times P(B \\mid A)          \\end{align}$$\n",
    "\n",
    "Our summand is the same as the values we've estimated either by guessing ($P(A)$) or from our data ($P(B \\mid A)$)! With that, we can rewrite our earlier equation as \n",
    "\n",
    "$$ P(A \\mid B) = \\frac{P(A) \\times P(B \\mid A)} {\\sum_{A \\in \\alpha} P(A) \\times P(B \\mid A)} $$\n",
    "\n",
    "With that, we can crunch the numbers and perform Bayesian inference like a champ or have a machine do it for us like a prudent delegator. Though for our everyday activities, we often don't have the luxury of having someone/something checking our heuristics. So it's always worth trying to keep in mind that oftentimes, many different factors that you may not know or take into consideration can culminate in observations that surprise you -- there's a good reason you aren't told to constantly get yourself tested for a medical condition if you don't believe to be at risk!\n",
    "\n",
    "\\- DK (4/24/18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization Problems\n",
    "\n",
    "Motivated to do this when I was reading [this paper](https://arxiv.org/pdf/1606.05579.pdf) and realized I forgot how we get to/use the KKT conditions (which is implied in Eq. 2 in the paper).\n",
    "\n",
    "## Equality constraints only: The Method of Lagrange Multipliers/The Langrangian\n",
    "\n",
    "NB: [Khan Academy's videos on the subject](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction) really make clear one potential intuition for one constraint.\n",
    "\n",
    "### Intuition with just one constraint\n",
    "\n",
    "Say you want to optimize (maximize or minimize) a smooth function $f(\\vec{x})$ subject to the constraint $g(\\vec{x}) = c$ (let's say $\\vec{x} \\in R^n$ -- and we may just write $x$ instead of $\\vec{x}$). (It's worth remembering that the constraints themselves can also be expressed as functions -- they just happen to be set to specific constants.) So our goal is to find the best point(s), ${\\vec{x}^*}$. For the purposes of explanation, we'll say our goal is maximization, but it all applies for minimization too.\n",
    "\n",
    "A way to build up the intuition is to consider the contour lines of $f(\\vec{x}) = m$ for particular values of m. Our goal then is to maximize $m$ while having $\\vec{x}$ satisfy $g(\\vec{x}) = c$. If it didn't fulfill this second requirement, then we'd just be ignoring the constraint and solving an unconstrained optimization problem -- in which case, we'd just set the gradient equal to zero and solve (say, what a useful thought -- let's put that in our back pocket for later...).\n",
    "\n",
    "Let's call the constraint contour line (which we aren't allowed to change and is set to some constant $c$) $G$ and the function contour line (which we can change by varying $m$) $F(m)$. If we think about it for a bit, we'll see that *solving our problem is analogous to choosing the largest value of $m$ so that $F(m)$ \"touches\" $G$ in the fewest number of places while still actually \"touching\" $G$.* Consider the alternatives:\n",
    "\n",
    "1. *$F(m)$ does not touch $G$ at all:* Well, that means that when we look at the set of points that comprise the contour line $f(x,y) = m$ (i.e., the **level set** of $f$ corresponding to a value of $m$), *none* of those points lie on $g(\\vec{x}) = c$. So we ignored the constraint again -- oops.\n",
    "2. *$F(m)$ touches $G$ too many times:* This implies that $F(m)$ cuts across $G$ (if it didn't, then where did the \"extra\" touches come from?) And in that case, why not increase $m$ a bit more? We're assuming $f$ is relatively well-behaved, so if you nudge $m$ up a bit to $m^+$, the contour line $F(m^+)$ will be fairly close to $F(m)$ -- and so, still cross $G$ somewhere.\n",
    "\n",
    "(By the way, I've been using the tortured phrase \"minimal number of times but not zero like c'mon don't be cheeky\" because there can be more than one location on the constraint curve that have the same maximal value for $f$. Consider maximizing $f(x,y) = x^2 y^2$ subject to $x^2 + y^2 = 1$. The symmetry leads to four points that satisfy the criteria -- bump up $f$'s output higher and you're off the circle, bump it down and you're cutting across the circle (and also not maximizing $f$).)\n",
    "\n",
    "Now, if $F(m)$ and $G$ just barely touch but do not cross, then their instantaneous \"slopes\" at their touching point must be in the same orientation and the contour lines are moving in the exact same \"direction\" -- if they weren't, then the touching point is a crossing point. So how do we capture this notion?\n",
    "\n",
    "### Doin' me some gradients\n",
    "\n",
    "The (nonzero) gradient of a function is always perpendicular to its contour lines. (Seems like a deep statement, but with a bit of thought, you can see that it just comes from the definition/intuition of contour lines (on which the value is constant) and gradients (which points toward the direction that increases the function output the largest -- with no portion of the \"step\" being wasted on movement that would keep the value constant).)\n",
    "\n",
    "So, we can convert out \"contour lines $F(m)$ and $G$ are in the same direction\" directly to \"the gradients of $f$ and $g$ are in the same direction\", i.e.\n",
    "$$\\nabla f = \\lambda \\nabla g $$\n",
    "\n",
    "where $\\nabla$ is the usual gradient operator and $\\lambda \\neq 0$, the scalar proportionality constant (it's the *direction* that matters, not the magnitude), is called the **Lagrange multiplier** (dude got a lot of things named after him).\n",
    "\n",
    "It's actually worth looking at this a bit more. We originally framed our goal by trying to get $F(m)$ and $G$ to touch as little as possible. But we can also frame it in terms of $\\nabla f$ and its relation to the constraint functions:\n",
    "\n",
    "$$\\nabla f = \\lambda \\nabla g \\quad \\impliedby \\quad \\nabla f \\ \\text{is perpendicular to the contour } G $$\n",
    "\n",
    "It makes sense that the right-hand side would also be the case -- if $\\nabla f$ *did* have some part of it along $G$, then we're wasting that part! Why wouldn't we go along $G$ a bit more? We'd still be meeting our constraint, and since we stepped partially along the direction of steepest ascent, we'd have increased $f$ while we were at it! It'll be worth remembering this observation a little down the line, so keep it in your pocket for later (or some other handy container, if you are doomed to the fate of clothing without functional storage capabilities).\n",
    "\n",
    "Anyway, that's great and all, but we only have $n$ equations (each of the $n$ elements of the vector formulation above) and now we have $(n+1)$ unknowns (all the coordinates for $x$, plus $\\lambda$). Well, there's a reason the above relation used a(n) $\\impliedby$. On the left-hand side, we've only captured that the gradients have to be in the same direction -- we haven't added our constraint! (The right-hand side encapsulates both, since we have $G$ as the contour corresponding to the specific constraint $g(x) = c$.) So our full set of $(n+1)$ equations with $(n+1)$ unknowns is\n",
    "\n",
    "$$\\nabla f = \\lambda \\nabla g $$\n",
    "$$g(x) = c$$\n",
    "\n",
    "Now go to town! Worth remembering that all of this provides *necessary* but **not** *sufficient* conditions for optimality. Sufficient conditions would involve, for example, proving the that Hessian matrix of $f$ is negative semidefinite when trying to maximize $f$ (analogous to the second-derivative test in the single-dimensional case), and even if you manage that you're only guaranteed local maximality. Sounds like a lot of qualifications, but we've actually narrowed the search space a great deal with these conditions, so it's not as terrible as it sounds.\n",
    "\n",
    "### The Lagrangian: a packaged function\n",
    "\n",
    "The above system of equations works great for people, but people have also spent so much time and energy to make computers solve math problems for us! Most of these programs are particular good and finding the zeros of a function (without any fancy constraints). So how could we repackage the (n+1) equations above into one function we can find into a zero-finder?\n",
    "\n",
    "Well, let's rewrite the above equations first:\n",
    "\n",
    "$$\\nabla f - \\lambda \\nabla g = 0$$\n",
    "$$g(x) - c = 0$$\n",
    "\n",
    "Alright, now what? Well, if we were to write something like\n",
    "\n",
    "$$ L(x) = f(x) - g(x) $$\n",
    "\n",
    "we'd be *almost* there, because if we took the gradient of $L$ and set it equal to zero, we get the \"direction\" constraint back. But at the moment, we're making it so that the magnitudes of the two gradients have to be the same too (which doesn't have to be true) *and* we forgot to incorporate our constraint again!\n",
    "\n",
    "Well, why don't we reintroduce $\\lambda$ as a variable in such a way that it handles the proportionality problem *and* have it so that  $L_{\\lambda} = g(x) - c$ (so that we reincorporate our constraint into the function)? Might sound tricky, but in fact we can modify $L$ to satisfy these requirements fairly simply:\n",
    "\n",
    "$$ \\mathcal{L}(x, \\lambda) = f(x) - \\lambda \\left(g(x) - c\\right) $$\n",
    "\n",
    "Now that it's achieved its final form (thankfully didn't take ten episodes of powering up), we change $L$ to $\\mathcal{L}$ and call it the **Lagrangian** of $f(x)$ (because dude needs more things named after him -- and you know, he *did* revolutionize the study of classical mechanics with this formulation).\n",
    "\n",
    "Worth noting is that if we define the Lagrangian as\n",
    "\n",
    "$$ \\mathcal{L^+}(x, \\lambda ^+) = f(x) + \\lambda^+\\left(g(x) - c\\right) $$\n",
    "\n",
    "we still get the same answer to our optimization problem -- the only difference is that compared with the $\\lambda$ we get from the $\\mathcal{L}$ formulation, $\\lambda ^+ = - \\lambda$.\n",
    "\n",
    "A neat consequence of the formulation of $\\mathcal{L}$ is that we can consider $\\lambda$ as a measure of how much we could improve $f(x)$ by incrementing the value of $c$ (which we've been considering a constant) by a differential amount. While it may seem to be \"clear\" just by taking $\\mathcal{L}_c = \\lambda$, it's a bit more subtle than that, since $\\mathcal{L}(x, \\lambda; c)$ was formulated with $c$ as a constant. The proof for this observation involves:\n",
    "\n",
    "- forming a new function, $\\mathcal{L}^*(x^*(c), \\lambda ^*(c), c) = \\mathcal{L}^*(c)$, a single-variable function that parameterizes the input coordinates of the answer(s) to the optimization problem (and also the Lagrange multiplier) with respect to $c$;\n",
    "- doing the multivariable chain rule;\n",
    "- thinking a bit to notice that a lot of things equal zero to get that $\\frac{d\\mathcal{L}^*}{dc} = \\lambda $ ;\n",
    "- and, having remembered that we're interested specifically about the points on $\\mathcal{L}$ that optimize $f$ (which is exactly what $\\mathcal{L}^*(c)$ captures), realizing that the above result implies that first statement of the paragraph before this bulleted list.\n",
    "\n",
    "What a mouthful.\n",
    "\n",
    "\n",
    "### Extension to more than one constraint\n",
    "\n",
    "Would be kind of a shame if we did all of this just to solve problems with just one constraint. But thankfully, the extension is fairly simple to describe!\n",
    "\n",
    "Say we want to optimize $f$ subject to $k$ constraints $g_1 = c_1, g_2 = c_2, \\cdots, g_k = c_k$. Now, in all but the most trivial of cases, it would be impossible to have the gradients of all of these different functions in the same direction. Intuitively (-ish, and assuming you feel comfortable-ish with concepts in linear algebra), if they can't all be in the same direction, you'd think that the \"next best thing\" would be that the gradient of $f$ is in the same direction as some linear combination of the gradients of $g_1, g_2, \\cdots, g_k$, i.e. that\n",
    "\n",
    "$$ \\nabla f = \\sum_{i=1}^k \\lambda _i \\nabla g_i $$\n",
    "\n",
    "That statement above is in fact a condition that is met in the answer to our optimization problem! (How convenient.)\n",
    "\n",
    "Now we have $n$ equations but $n+k$ unknowns. We once again fix that by actually incorporating our constraints:\n",
    "\n",
    "$$ \\nabla f = \\sum_{i=1}^k \\lambda _i \\nabla g_i $$\n",
    "$$ g_1 = c_1 $$\n",
    "$$ ... $$\n",
    "$$ g_k = c_k $$\n",
    "\n",
    "We can once again package everything together as a Lagrangian by having that $\\mathcal{L}_{\\lambda _i} = g_i - c_i$:\n",
    "\n",
    "$$\\begin{align} \\mathcal{L}(x, \\lambda _1, \\cdots, \\lambda _k) &= \n",
    "                                            f - (\\lambda _1 (g_1 - c_1) + \\cdots + \\lambda _k (g_k - c_k) ) \\\\\n",
    "                     &= f - \\sum_{i=1}^k \\lambda _i (g_i - c_i) \\end{align}$$\n",
    "\n",
    "And Bob's your uncle.\n",
    "\n",
    "#### Explaining the convenience\n",
    "\n",
    "Earlier we just kind of accepted the convenience of our guess, but it's worth figuring out why it works. Remember that observation you kept in your pocket (or other handy container)?\n",
    "\n",
    "$$\\nabla f = \\lambda \\nabla g \\quad \\impliedby \\quad \\nabla f \\ \\text{is perpendicular to the contour } G $$\n",
    "\n",
    "If $\\nabla f$ had any part of it along $G$, then we could step along $G$ and further increase $f$. This sounds extensible to more than one constraint! And in fact, the \"convenient\" result captures this for the contour line created by the intersection of all the constraints:\n",
    "\n",
    "$$\\nabla f = \\sum_{i=1}^k \\lambda _i \\nabla g_i  \\quad \\impliedby\n",
    "                        \\quad \\nabla f \\ \\text{is perpendicular to the intersection of all constraint contours } \\bigcap _i G_i $$\n",
    "\n",
    "Here, $\\bigcap _i G_i$ serves as the one contour line on which we can move along that satisfies all the constraints. (Presumably such continuous arcs exist -- otherwise, there are only discontinuities (i.e. discrete points), and so each point in the set would have to be checked individually.)\n",
    "\n",
    "Now, since we're dealing with the same construct (a contour along which we can move that satisfies the constraints), the same reasoning applies pretty much verbatim -- if $\\nabla f$ weren't perfectly perpendicular to $\\bigcap _i G_i$, we'd be wasting the component of the gradient going along $\\bigcap _i G_i$, $\\nabla f ^{\\parallel}$. So we'd just step along the contour and improve our result.\n",
    "\n",
    "So that explains why the right-hand side makes sense. But how does that imply the left-hand side? Specifically, how do we get the linear combination part, $ \\sum_{i=1}^k \\lambda _i \\nabla g_i $?\n",
    "\n",
    "Well, since we're all on contour lines at the same time, $\\bigcap _i G_i$ is necessarily perpendicular to all the gradients $\\nabla g_i$. Then any linear combination $\\sum_{i=1}^k \\lambda _i \\nabla g_i $ is perpendicular to $\\bigcap _i G_i$. In fact, the gradients form a *basis* for the space perpendicular to $\\bigcap _i G_i$, because of the symmetric property of the perpendicularity relation. More curtly, call the span of the gradients $S$. By construction, $\\bigcap _i G_i \\perp S$, and by symmetry, S \\perp $\\bigcap _i G_i$.) We want $\\nabla f$ to be perpendicular to $\\bigcap _i G_i$ (as we've said before). Well then, that means $\\nabla f \\in S$, which implies that it $\\nabla f$ can be written as a linear combination of $S$'s basis vectors, i.e., $ \\nabla f = \\sum_{i=1}^k \\lambda _i \\nabla g_i $. Bam! Stick that beautiful $Q.E.D.$ square in the corner, we are done! (Would do it myself were I writing this in $LaTeX$ and not Markdown.)\n",
    "\n",
    "\n",
    "\\- DK, 4/28/18\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints with Inequalities: the Karush-Kuhn-Tucker (KKT) Conditions\n",
    "\n",
    "### Not nearly as scary as they make it out to be.\n",
    "\n",
    "For all the pomp and circumstance around this, with the caravan of names in the name itself and the esoteric terms used in its description like \"complementary slackness\" and \"dual feasibility\", the Karush-Kuhn-Trucker (KKT) conditions aren't nearly as hard to follow as one would expect if the method of Lagrange multipliers for multiple constraints makes sense/is comfortable.\n",
    "\n",
    "First, we pose the optimization problem in \"standard form\" (which mainly just saves us from lugging around extra constants like we did with $c$ in the Lagrange multipliers explanation):\n",
    "\n",
    "Optimize $f(x)$ subject to $g_i(x)\\leq 0$, $h_j(x)=0$, with $i \\in \\{1, \\cdots, k\\}$ and $j \\in \\{1, \\cdots, l\\}$ (so $k$ inequality constraints and $l$ equality constraints). Below, we'll assume \"optimize\" = \"maximize\". We'll point out where changes will occur if you're minimizing instead.\n",
    "\n",
    "#### Primal feasibility\n",
    "\n",
    "This time, before we do anything else, we're going to stick down the original constraints so we don't forget they exist:\n",
    "\n",
    "$$ g_i(x)\\leq 0 \\ \\forall \\ i, \\ h_j(x) = 0 \\ \\forall \\ j $$\n",
    "\n",
    "This is called the **primal feasibility** condition because it's a condition for the feasibility of the original, main, \"primal\" problem.\n",
    "\n",
    "### The dual formulation\n",
    "\n",
    "We refer to the original problem as the \"primal\" problem to contrast it with the \"dual\" problem. That sounds all fancy, but we've made a dual problem before when we formed the Lagrangian for our equality-constraints-only version. That is, the dual problem is simply a reframed but equivalent form of the primal problem. We did it before by solving a function that had all our constraints wrapped in one clean package (the Lagrangian form of the problem). And hey, that was both a neat *and* a useful idea, and those don't come around all that often, so let's use it until it goes out of style.\n",
    "\n",
    "Worth noting is that we want to make our dual problem mirror the primal problem exactly (in *optimal value* as well as optimal location), i.e. form a strong duality, i.e. have no [duality gap](https://en.wikipedia.org/wiki/Duality_gap). The following provide *necessary* conditions, but not *sufficient* conditions for a strong duality.\n",
    "\n",
    "So what would be the dual problem? We can do the same thing we did for the Lagrangian -- make a new function with some extra variables whose partial derivatives yield the constraints of our problem. Let's try it:\n",
    "\n",
    "We define a function\n",
    "\n",
    "$$L(x,\\mu_1, \\cdots, \\mu_k, \\lambda_1, \\cdots, \\lambda_l) = f(x) + \\sum_i \\mu_i g_i(x) + \\sum_j \\lambda_j h_j(x)$$. \n",
    "\n",
    "Maximize $L$ (with no external constraints; i.e., find the locations where $\\nabla L = \\vec{0}$). (We'll come back to minimization later.)\n",
    "\n",
    "Alright, well that's certainly something. Now we can recover the constraints by noting that $L_{\\mu_i} = g_i$ and $L_{\\lambda_j} = h_j$. But there are some things that are still funky with the inequality constraints here, so let's work on those.\n",
    "\n",
    "(By the way, there are some annoying flipping of signs if we\n",
    "\n",
    "#### Dual feasibility\n",
    "\n",
    "For one thing, our dual problem won't mimic our primal problem of optimizing $f$ at all if we let any $\\mu_i$ be less than zero. If we did, then we'd easily \"win\" the optimization game by choosing some $x$ such that $g_i(x) < 0$ (which is still satisfies our primal feasibility conditions), then setting the corresponding $\\mu_i$ to arbitrariliy large negative numbers -- who cares about $f(x)$ anyway!? Oh wait, we do. So we should probably make sure that we can't break our problem:\n",
    "\n",
    "$$ \\mu_i \\geq 0 $$\n",
    "\n",
    "This is called the **dual feasibility** condition because, well, otherwise our dual problem isn't all that useful.\n",
    "\n",
    "#### Complementary slackness and stationarity\n",
    "\n",
    "For one thing, we can notice that there are two possibilities for the value $g_i$ takes at an optimal point:\n",
    "\n",
    "1. $g_i(x^*) < 0$: That means the inequality constraint is not actually stopping the function from getting to a \"better\" location where $g_i > 0$ (our optimal-point finder never hit a wall -- there's an open neighborhood around $x^*$ that still satisfies the constraint), so the constraint isn't being restrictive at all. So we don't have to worry about it!\n",
    "2. $g_i(x^*) = 0$: The inequality constraint *is* potentially stopping the function from reaching a more optimal point, so the constraint is actually affecting the outcome. So we should be sure to be othorgonal to the contour in our final answer.\n",
    "\n",
    "These observations inform the following two constraints, **complementary slackness** and **stationarity**:\n",
    "\n",
    "$$\\begin{align} \\mu_i g_i = 0 \\ \\forall \\ i \\qquad & \\text{complementary slackness} \\\\\n",
    "\\nabla f = \\sum _i \\mu _i \\nabla g_i + \\sum _j \\lambda _j \\nabla h_j \\qquad & \\text{stationarity} \\end{align}$$\n",
    "\n",
    "Let's once again consider our two cases:\n",
    "\n",
    "1. $g_i(x^*) < 0$: This constraint is inactive and so is not part of the contour lines we use to find the set of points on which $f$ must lie. Recall that we form this set by evaluating the intersection of all the active constraints $C$; that the span of the gradients of the active constraint functions form a space $S$ orthogonal to $C$; and that since $\\nabla f$ must be orthogonal to $C$ in order to be a potential extremum, $\\nabla f \\in S$ and can be written as a linear combination of the gradients of the active constraints. (Boy, another mouthful.) Well, $g_i$ is not an active constraint, and so $\\nabla g_i$ is not part of the basis for $S$. So we want its contribution in the stationary condition to be $\\vec{0}$. We can do this by setting the corresponding $\\mu_i := 0$. Not-so-coincidentally, this necessary consequence leads to compliance with the complementary slackness condition as well.\n",
    "2. $g_i(x^*) = 0$: The complementary slackness condition is met no matter what value $\\mu_i$ takes. And that's perfect: the constraint is active, so $\\nabla g_i$ is part of the basis for $S$. So we *need* $\\mu_i$ to be nonzero so that we can describe any vector in $S$ (of which $\\nabla f$ is an element, as we talked about when we \"explained the convenience\" for the multiple-equality-constraint formulation earlier).\n",
    "\n",
    "So the two conditions are tied to one another in this interesting way that makes it a more-or-less direct extension of the multiple-equality-constraint formulation!\n",
    "\n",
    "\n",
    "#### Finishing our duel with duals.\n",
    "\n",
    "...And with that, we've formed our dual problem with only equalities! We'll copy them here to show we have enough equations for our unknowns:\n",
    "\n",
    "$$\\begin{align} h_j(x) = 0 \\qquad            & \\text{primal feasibility} \\\\\n",
    "\\mu_i g_i = 0 \\ \\forall \\ i \\qquad           &\\text{complementary slackness} \\\\\n",
    "\\nabla f = \\sum _i \\mu _i \\nabla g_i + \\sum _j \\lambda _j \\nabla h_j \\qquad & \\text{stationarity}\\end{align}$$\n",
    "\n",
    "That's $k + l + n$ equations for $k + l + n$ unknowns! Now stick the system of equations into a (probably numerical) zero-finder and you're done!\n",
    "\n",
    "#### On minimizing $f$\n",
    "\n",
    "Let's not forget that we had done all this assuming we were *maximizing* $L$ (and thus $f$). If we were *minimizing* $f$, we would be trying to minimize $L$ and we could change the above equations in one of the following ways:\n",
    "\n",
    "1. Replace $L$ with $L(x,\\mu_1, \\cdots, \\mu_k, \\lambda_1, \\cdots, \\lambda_l) = f(x) - \\sum_i \\mu_i g_i(x) - \\sum_j \\lambda_j h_j(x)$ (note the minus signs). Replace the stationarity condition with $-\\nabla f = \\sum _i \\mu _i \\nabla g_i + \\sum _j \\lambda _j \\nabla h_j$ (again, note the minus sign).\n",
    "2. Keep $L$ the same. Replace the dual feasibility condition with $ \\mu_i \\leq 0 $\n",
    "\n",
    "The sign changes just ensure that our dual problem is still well-formed for the minimization problem (if you don't flip the sign somewhere, then we can just make arbitrarily \"good\" values by playing with $\\mu_i$ again; and if you flip the sign in the formulation of $L$, you have to make sure you update the gradient expression accordingly (Option 1)).\n",
    "\n",
    "### Why did you talk so much?\n",
    "\n",
    "...OK, so maybe this was a lot more to explain than I had given credit at first. But it's really not that bad! The main intuition is that:\n",
    "1. either the inequality constraint is lax and doesn't actually do any \"constraining\"; or \n",
    "2. the inequality constraint is actually stopping us, in which case it just becomes another equality constraint!\n",
    "\n",
    "All the blah-blah-blah is to make sure we dotted our $i$'s and crossed out $t$'s. (And boy, did we...)\n",
    "\n",
    "\\- DK, 4/30/18 (shoot, when did it get so late...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drafts and WIPs\n",
    "\n",
    "(Avert your eyes!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "## Bayes' Theorem\n",
    "\n",
    "I can never seem to remember Bayes' Theorem directly, as they write it out in textbooks. It makes so much more sense to me to think about it from the relationships between conditional and joint probabilites/distributions, and one of the common tricks to make Bayes' Theorem useful in practice also comes to me far more easily when explicitly thinking about events as being sampled from a *sample space* of possibilities/outcomes. (I find that when I read \"consider two possible events A and B\", I tend to forget that A can be one of several different events, so hopefully the discussion below is clearer in that regard.)\n",
    "\n",
    "// (like how sampling a \"number\" from a samples from a distribution instead of events (which is sampled from an event space, but like, who immediately thinks about events as having been sampled from an \"event space\"? (Statisticians I'd wager, but probably not too many other people.)).\n",
    "\n",
    "So, let's talk about the weather. (Sorry if the topic is a bit dry by this point.)\n",
    "\n",
    "Specifically, let's consider a forecast as described by two variables: type of coverage (\"sunny\", \"cloudy\", \"thunderstorms\", ...), which we'll call $C$, and average temperature for the day (in, say, degrees Fahrenheit and rounded to the nearest integer), which we'll call $T$. These are certainly random variables -- one has certainly cursed their weather app for faulty information when caught out in the rain without an umbrella. But they also definitely have some connection with one another -- you'd probably be surprised to find it snowing if you look up and it's a clear sunny day (if not, you'll have to tell me where such an odd sight takes place).\n",
    "\n",
    "We can consider the **sample space** of weather outcomes $W$, i.e. the space that contains all possible weather combinations, as being composed of two \"axes\", one being the set of all types of coverage (let's call that $C$) and the other being the set of all temperatures readings (let's call that $T$). So we can *kind of* think of things as:\n",
    "\n",
    "$$ W = C \\times T $$\n",
    "\n",
    "with $C \\times T$ being the Cartesian product of $C$ and $T$. I put all these (written) verbal asterisks because I'm stretching some definitions and not being super thorough: \"axes\" imply ordering, which doesn't always make sense (what comes after \"cloudy\" on the $C$ axis?) and isn't necessary; and the elements of $W$ needn't be tuples, unlike what a Cartesian product implies (and I'm also not sure whether one can actually apply the Cartesian product to random variables).\n",
    "\n",
    "All that said, each point $w \\in W$ describes a particular weather condition and has an associated probability. If we consider a particular weather condition with $w = (c,t), c \\in C, t \\in T$, we can write:\n",
    "\n",
    "$$ \\text{Pr[W = w]} = \\text{Pr[C = c and T = t]} $$\n",
    "\n",
    "Here we're following the convention that $\\text{Pr[X = x]}$ is the probability that the random variable $X$ takes on the value $x$. We'll also follow the convention where $\\text{Pr[X = x]} = P(x)$. So we'll write the above as:\n",
    "\n",
    "$$ P(w) = P(c,t) $$\n",
    "\n",
    "Now, being high-tech cosmologists, we have a way of sending a probe into the future and getting back information on either the sky coverage or the temperature. )\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

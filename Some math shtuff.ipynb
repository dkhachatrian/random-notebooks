{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some math shtuff\n",
    "\n",
    "A quick(ish) reference for some concepts whose intuitions/simple methods of understanding sometimes escape me.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy, Cross-Entropy, KL Divergence\n",
    "\n",
    "There are a number of ways to think about/get to the Shannon entropy ($H$) (thankfully not $S$...) One simple way is to say what we want it to mean qualitatively and impose some desired characteristics to determine its mathematical formulation. In this case, it may make more sense to start with cross-entropy first.\n",
    "\n",
    "Say you're sending me messages from an alphabet of symbols $A$. Some are more likely than others, and we'll call the probability distribution associated with the actual generator of symbols $p$. I'm over here on the other side thinking like I'm a pair of smartypants ~~and start looking to spread my brand through nationwide department stores~~ that has figured out how likely you are to send each symbol to me. We'll call my expected distribution (which may or may not be the correct distribution) $q$.\n",
    "\n",
    "Now we want to measure how surprised I am by any given message you send me -- let's call it $\\tau$ (rhymes with \"wow\" -- alas, $\\tau$ is not standard notation). We would imagine the following properties would be useful for $\\tau$ to have:\n",
    "1. The more surprised I am, the larger $\\tau$ gets (otherwise, it wouldn't exactly be doing a good job measuring my surprise)\n",
    "2. If symbols are independently generated, we can construct the total surprise of my message by adding the total surprise of each symbol in the message, i.e., $\\tau_{M} = \\sum_{i=1}^{m} \\tau_{M[i]}$ where $M \\in A^m$ is a string of symbols, $m$ is the cardinality of $M$, and $M[i]$ denotes the $i$'th element of $M$.\n",
    "\n",
    "Using these two criteria, a reasonable mapping is\n",
    "\n",
    "$$\\tau_{a} = \\log\\left(\\frac{1}{q_a}\\right), a \\in A$$\n",
    "\n",
    "where $q_a$ is the probability I think you'll send the symbol $a$.\n",
    "\n",
    "I mean, that's great and all, but that works for individual instances of strings or symbols. How much should I *expect* to be surprised by any given symbol? Well, that'd depend on how often you *actually* send me a symbol, alongside how often I expect to receive that particular symbol.\n",
    "\n",
    "Hmm, this smells of expectation! And indeed, that's all we do -- take the expectation over $A$ (using the *actual* probability of each symbol occurring, i.e., using $p$) of my surprise per symbol:\n",
    "\n",
    "$$E_{a \\sim p}[\\tau] = \\sum_{a \\in A} p_a \\log\\left(\\frac{1}{q_a}\\right) = H(p,q)$$\n",
    "\n",
    "We denote this metric $H(p,q)$ (where $p$ and $q$ are the actual and presumed distributions, respectively) and call it the **cross-entropy** (because that sounds pretty cyberpunk to me. I like to think they throw in \"cross\" because it measures the surprise caused by \"crossing\" the distributions $p$ and $q$ together.)\n",
    "\n",
    "Now, we'd imagine that I'd be the least surprised if my presumed distribution of symbols were in fact the actual distribution, i.e. when $p = q$. In fact, this is true!\n",
    "\n",
    "$$H(p,p) = H(p) = \\sum_{a \\in A} p_a \\log\\left(\\frac{1}{p_a}\\right)$$\n",
    "\n",
    "is called the **entropy** (or **Shannon entropy**) of the probability distribution and written $H$ (thankfully not $S$...). Usually, the $log$s written above are base-2. This permits a way of thinking of the value of the Shannon entropy: if I'm only allowed to ask the same series of questions to you to figure out which symbol you want to send me and I know the actual probability distribution $p$ of symbols, how many questions should I expect to ask (i.e. mean/average) before I figure out the answer? The cross-entropy is the same thing, expect I don't necessarily know the actual probability distribution $p$ of symbols, I just think I do (and I think it's $q$) and base my series of questions based on $q$.\n",
    "\n",
    "Now, hopefully that makes it clear that $ p \\neq q \\implies H(p,q) > H(p) $ -- if I don't know the actual distribution, I'm not going to be able to answer the most optimal series of questions. This suggestions to us a notion of \"distance\" (i.e. a metric) between the probability distributions $p$ and $q$:\n",
    "\n",
    "$$ KL(p \\mid\\mid q) = H(p,q) - H(p) $$\n",
    "\n",
    "This metric is called the **Kullbackâ€“Leibler divergence** (because names) or the **KL divergence** (because initialisms), or seemingly most rarely but probably most clearly the **relative entropy**. Out loud, you'd say $KL(p \\mid\\mid q)$ is \"the \\[blah\\] of p with respect to q\". The closer the KL divergence is to 0, the closer $q$ is to being $p$, and $ KL(p \\mid\\mid q) = 0 \\implies p = q $ at every point in the domain of $q$ (which is the same as the domain of $p$).\n",
    "\n",
    "This makes describing the **mutual information** of random variables X and Y in terms of a KL divergence fairly intuitive. Just running off the name, if X and Y were independent, you'd expect no mutual information between them. In such a case, $P(X,Y) = P(X)P(Y)$, and $ KL\\left(P(X,Y)\\mid\\mid P(X)P(Y)\\right) = 0$. And in fact, one way of writing the mutual information of random variables X and Y is exactly\n",
    "\n",
    "$$ I(X;Y) = KL\\left(P(X,Y)\\mid\\mid P(X)P(Y)\\right) $$\n",
    "\n",
    "which I think is pretty neat.\n",
    "\n",
    "\\- DK (4/24/18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "I can never seem to remember Bayes' Theorem directly, as they write it out in textbooks. It makes so much more sense to me to think about it from the relationships between conditional and joint probabilites/distributions, and one of the common tricks to make Bayes' Theorem useful in practice also comes to me far more easily when explicitly thinking about events as being sampled from a *sample space* of possibilities/outcomes.\n",
    "\n",
    "\n",
    "\n",
    "Consider two possible events A and B. Let's keep in mind that A is just one possible outcome out of a set of possibilities, as is B; we'll say $\\alpha$ is the set of possibilities from which $A$ was drawn and $\\beta$ is the set of possibilities from which $B$ was drawn, i.e. $A \\sim \\alpha$ and $B \\sim \\beta$ (this will be good to remember later). Now:\n",
    "\n",
    "$$ \\text{Pr[A and B both occur]} := P(A,B) $$\n",
    "\n",
    "Assuming individual events happen separately, there are two ways for both A and B to occur:\n",
    "1. A happens first, then B happens.\n",
    "2. B happens first, then A happens.\n",
    "\n",
    "(\"Duh\", I know.)\n",
    "\n",
    "Keeping in mind that the first event might affect the probability of the second event occurring (i.e. remembering that conditional probabilities exist), we can write:\n",
    "\n",
    "$$ P(A,B) = P(A) \\times P(B \\mid A) = P(B) \\times P(A \\mid B) $$\n",
    "\n",
    "And then it's simple to write out Bayes' Theorem as it's often written (we'll write it perhaps a bit more evocatively):\n",
    "\n",
    "$$ P(B) \\times P(A \\mid B) = P(A) \\times P(B \\mid A) $$\n",
    "\n",
    "$$\\begin{align} P(A \\mid B) &= \\frac{P(A) \\times P(B \\mid A)} {P(B)} \\\\\n",
    "                        &= P(A) \\times \\frac {P(B \\mid A)} {P(B)} \\end{align}$$\n",
    "\n",
    "Using the Bayesian interpretation: At first we thought the probability that $A$ occurs is $P(A)$. After we saw that $B$ happened, we re-evaluate the probability that $A$ occurs with a scaling factor $ \\frac {P(B \\mid A)} {P(B)} $\n",
    "\n",
    "Also worth knowing the fancy terminology:\n",
    "- the **_a priori_ probability** or just the **prior** is what we thought would be the probability that a random variable takes on a certain value before we observed anything. So the *a priori* probability (or just prior) for the event $A$ would be $P(A)$. If we consider $A$ to be a random variable instead of an event, we're guessing the distribution of $A$ and so $P(A)$ would be an **_a priori_ distribution** (or again, just the prior).\n",
    "- the **_a posteriori_ probability** or just the **posterior** is what we think the probability that a random variable takes on a certain value is after observing something. In this case, the *a posteriori* probability (or just posterior) of the event $A$ after observing $B$ is $P(A \\mid B)$. If we consider $A$ to be a random variable instead of an event, we're guessing the distribution of $A$ after observing a random variable/event B and so $P(A \\mid B)$ would be an **_a posteriori_ distribution**, (or again, just the posterior).\n",
    "\n",
    "\n",
    "Now, in cases of inference, we normally have some data on the likelihood of one of the conditionals -- let's say $P(B \\mid A)$ to avoid flipping our equation around again -- via empirical counts. So we've already guessed some prior $P(A)$, and we're trying to improve it by calculating the posterior $ P(A \\mid B) $. But what if we don't know $ P(B) $? Is our guessing and data collection all for naught!? Thankfully, not so! The answer lies right under our noses -- or in this case, in our previous calculations.\n",
    "\n",
    "Consider $P(A,B)$ again. What would we get if we added $P(A,B)$ over all possible values of A? (Remember we said that $A \\sim \\alpha$, so A could have been some other event within the set $\\alpha$.) That's basically just saying that we don't care what value $A$ is, so we end up with $P(B)$! ('!' used to denote excitement, not factorialization.) And conveniently, we'd already have a way to estimate these values:\n",
    "\n",
    "$$\\begin{align} P(B) &= \\sum_{A \\in \\alpha} P(A,B) \\\\\n",
    "                     &= \\sum_{A \\in \\alpha} P(A) \\times P(B \\mid A)          \\end{align}$$\n",
    "\n",
    "Our summand is the same as the values we've estimated either by guessing ($P(A)$) or from our data ($P(B \\mid A)$)! With that, we can rewrite our earlier equation as \n",
    "\n",
    "$$ P(A \\mid B) = \\frac{P(A) \\times P(B \\mid A)} {\\sum_{A \\in \\alpha} P(A) \\times P(B \\mid A)} $$\n",
    "\n",
    "With that, we can crunch the numbers and perform Bayesian inference like a champ or have a machine do it for us like a prudent delegator. Though for our everyday activities, we often don't have the luxury of having someone/something checking our heuristics. So it's always worth trying to keep in mind that oftentimes, many different factors that you may not know or take into consideration can culminate in observations that surprise you -- there's a good reason you aren't told to constantly get yourself tested for a medical condition if you don't believe to be at risk!\n",
    "\n",
    "\\- DK (4/24/18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drafts and WIPs\n",
    "\n",
    "(Avert your eyes!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "## Bayes' Theorem\n",
    "\n",
    "I can never seem to remember Bayes' Theorem directly, as they write it out in textbooks. It makes so much more sense to me to think about it from the relationships between conditional and joint probabilites/distributions, and one of the common tricks to make Bayes' Theorem useful in practice also comes to me far more easily when explicitly thinking about events as being sampled from a *sample space* of possibilities/outcomes. (I find that when I read \"consider two possible events A and B\", I tend to forget that A can be one of several different events, so hopefully the discussion below is clearer in that regard.)\n",
    "\n",
    "// (like how sampling a \"number\" from a samples from a distribution instead of events (which is sampled from an event space, but like, who immediately thinks about events as having been sampled from an \"event space\"? (Statisticians I'd wager, but probably not too many other people.)).\n",
    "\n",
    "So, let's talk about the weather. (Sorry if the topic is a bit dry by this point.)\n",
    "\n",
    "Specifically, let's consider a forecast as described by two variables: type of coverage (\"sunny\", \"cloudy\", \"thunderstorms\", ...), which we'll call $C$, and average temperature for the day (in, say, degrees Fahrenheit and rounded to the nearest integer), which we'll call $T$. These are certainly random variables -- one has certainly cursed their weather app for faulty information when caught out in the rain without an umbrella. But they also definitely have some connection with one another -- you'd probably be surprised to find it snowing if you look up and it's a clear sunny day (if not, you'll have to tell me where such an odd sight takes place).\n",
    "\n",
    "We can consider the **sample space** of weather outcomes $W$, i.e. the space that contains all possible weather combinations, as being composed of two \"axes\", one being the set of all types of coverage (let's call that $C$) and the other being the set of all temperatures readings (let's call that $T$). So we can *kind of* think of things as:\n",
    "\n",
    "$$ W = C \\times T $$\n",
    "\n",
    "with $C \\times T$ being the Cartesian product of $C$ and $T$. I put all these (written) verbal asterisks because I'm stretching some definitions and not being super thorough: \"axes\" imply ordering, which doesn't always make sense (what comes after \"cloudy\" on the $C$ axis?) and isn't necessary; and the elements of $W$ needn't be tuples, unlike what a Cartesian product implies (and I'm also not sure whether one can actually apply the Cartesian product to random variables).\n",
    "\n",
    "All that said, each point $w \\in W$ describes a particular weather condition and has an associated probability. If we consider a particular weather condition with $w = (c,t), c \\in C, t \\in T$, we can write:\n",
    "\n",
    "$$ \\text{Pr[W = w]} = \\text{Pr[C = c and T = t]} $$\n",
    "\n",
    "Here we're following the convention that $\\text{Pr[X = x]}$ is the probability that the random variable $X$ takes on the value $x$. We'll also follow the convention where $\\text{Pr[X = x]} = P(x)$. So we'll write the above as:\n",
    "\n",
    "$$ P(w) = P(c,t) $$\n",
    "\n",
    "Now, being high-tech cosmologists, we have a way of sending a probe into the future and getting back information on either the sky coverage or the temperature. )\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization Problems\n",
    "\n",
    "## Equality constraints only: The Method of Lagrange Multipliers/The Langrangian\n",
    "\n",
    "### Intuition with just one constraint\n",
    "\n",
    "Say you want to optimize (maximize or minimize) a (piecewise?) differentiable function $f(\\vec{x})$ subject to the constraint $g(\\vec{x}) = c$ (let's say $\\vec{x} \\in R^n$ -- and we may just write $x$ instead of $\\vec{x}$). (It's worth remembering that the constraints themselves can also be expressed as functions -- they just happen to be set to specific constants.) So our goal is to find the best point(s), ${\\vec{x}^*}$. For the purposes of explanation, we'll say our goal is maximization, but it all applies for minimization too.\n",
    "\n",
    "A way to build up the intuition is to consider the contour lines of $f(\\vec{x}) = m$ for particular values of m. Our goal then is to maximize $m$ while having $\\vec{x}$ satisfy $g(\\vec{x}) = c$. If it didn't fulfill this second requirement, then we'd just be ignoring the constraint and solving an unconstrained optimization problem -- in which case, we'd just set the gradient equal to zero and solve (say, what a useful thought -- let's put that in our back pocket for later...).\n",
    "\n",
    "Let's call the constraint contour line (which we aren't allowed to change and is set to some constant $c$) $G$ and the function contour line (which we can change by varying $m$) $F(m)$. If we think about it for a bit, we'll see that *solving our problem is analogous to choosing the largest value of $m$ so that $F(m)$ \"touches\" $G$ in the fewest number of places while still actually \"touching\" $G$.* Consider the alternatives:\n",
    "\n",
    "1. *$F(m)$ does not touch $G$ at all:* Well, that means that when we look at the set of points that comprise the contour line $f(x,y) = m$ (i.e., the **level set** of $f$ corresponding to a value of $m$), *none* of those points lie on $g(\\vec{x}) = c$. So we ignored the constraint again -- oops.\n",
    "2. *$F(m)$ touches $G$ too many times:* This implies that $F(m)$ cuts across $G$ (if it didn't, then where did the \"extra\" touches come from?) And in that case, why not increase $m$ a bit more? We're assuming $f$ is relatively well-behaved, so if you nudge $m$ up a bit to $m^+$, the contour line $F(m^+)$ will be fairly close to $F(m)$ -- and so, still cross $G$ somewhere.\n",
    "\n",
    "(By the way, I've been using the tortured phrase \"minimal number of times but not zero like c'mon don't be cheeky\" because there can be more than one location on the constraint curve that have the same maximal value for $f$. Consider maximizing $f(x,y) = x^2 y^2$ subject to $x^2 + y^2 = 1$. The symmetry leads to four points that satisfy the criteria -- bump up $f$'s output higher and you're off the circle, bump it down and you're cutting across the circle (and also not maximizing $f$).)\n",
    "\n",
    "Now, if $F(m)$ and $G$ just barely touch but do not cross, then their instantaneous \"slopes\" at their touching point must be in the same orientation and the contour lines are moving in the exact same \"direction\" -- if they weren't, then the touching point is a crossing point. So how do we capture this notion?\n",
    "\n",
    "### Doin' me some gradients\n",
    "\n",
    "The (nonzero) gradient of a function is always perpendicular to its contour lines. (Seems like a deep statement, but with a bit of thought, you can see that it just comes from the definition/intuition of contour lines (on which the value is constant) and gradients (which points toward the direction that increases the function output the largest -- with no portion of the \"step\" being wasted on movement that would keep the value constant).)\n",
    "\n",
    "So, we can convert out \"contour lines $F(m)$ and $G$ are in the same direction\" directly to \"the gradients of $f$ and $g$ are in the same direction\", i.e.\n",
    "$$\\nabla f = \\lambda \\nabla g $$\n",
    "\n",
    "where $\\nabla$ is the usual gradient operator and $\\lambda \\neq 0$, the scalar proportionality constant (it's the *direction* that matters, not the magnitude), is called the **Lagrange multiplier** (dude got a lot of things named after him).\n",
    "\n",
    "That's great and all, but now we have $n$ equations (each of the $n$ elements of the vector formulation above) and $(n+1)$ unknowns (all the coordinates for $x$, plus $\\lambda$). Well, we only captured that the gradients have to be in the same direction -- we haven't added our constraint! So our full set of $(n+1)$ equations with $(n+1)$ unknowns is\n",
    "\n",
    "$$\\nabla f = \\lambda \\nabla g $$\n",
    "$$g(x) = c$$\n",
    "\n",
    "Now go to town!\n",
    "\n",
    "### The Lagrangian: a packaged function\n",
    "\n",
    "The above works great for people, but people have also spent so much time and energy to make computers solve math problems for us! Most of these programs are particular good and finding the zeros of a function (without any fancy constraints). So how could we repackage the (n+1) equations above into one function we can find into a zero-finder?\n",
    "\n",
    "Well, let's rewrite the above equations first:\n",
    "\n",
    "$$\\nabla f - \\lambda \\nabla g = 0$$\n",
    "$$g(x) - c = 0$$\n",
    "\n",
    "Alright, now what? Well, if we were to write something like\n",
    "\n",
    "$$ L(x) = f(x) - g(x) $$\n",
    "\n",
    "we'd be *almost* there, because if we took the gradient of $L$ and set it equal to zero, we get the \"direction\" constraint back. But at the moment, we're making it so that the magnitudes of the two gradients have to be the same too (which doesn't have to be true) *and* we forgot to incorporate our constraint again!\n",
    "\n",
    "Well, why don't we reintroduce $\\lambda$ as a variable in such a way that it handles the proportionality problem *and* have it so that  $L_{\\lambda} = g(x) - c$ (so that we reincorporate our constraint into the function)? Might sound tricky, but in fact we can modify $L$ to satisfy these requirements fairly simply:\n",
    "\n",
    "$$ \\mathcal{L}(x, \\lambda) = f(x) - \\lambda \\left(g(x) - c\\right) $$\n",
    "\n",
    "Now that it's achieved its final form (thankfully didn't take ten episodes of powering up), we change $L$ to $\\mathcal{L}$ and call it the **Lagrangian** of $f(x)$ (because dude needs more things named after him -- and you know, he *did* revolutionize the study of classical mechanics with this formulation).\n",
    "\n",
    "Worth noting is that if we define the Lagrangian as\n",
    "\n",
    "$$ \\mathcal{L^+}(x, \\lambda ^+) = f(x) + \\lambda^+\\left(g(x) - c\\right) $$\n",
    "\n",
    "we still get the same answer to our optimization problem -- the only difference is that compared with the $\\lambda$ we get from the $\\mathcal{L}$ formulation, $\\lambda ^+ = - \\lambda$.\n",
    "\n",
    "A neat consequence of the formulation of $\\mathcal{L}$ is that we can consider $\\lambda$ as a measure of how much we could improve $f(x)$ by incrementing the value of $c$ (which we've been considering a constant) by a differential amount. While it may seem to be \"clear\" just by taking $\\mathcal{L}_c = \\lambda$, it's a bit more subtle than that, since $\\mathcal{L}(x, \\lambda; c)$ was formulated with $c$ as a constant. The proof for this observation involves:\n",
    "\n",
    "- forming a new function, $\\mathcal{L}^*(x^*(c), \\lambda ^*(c), c) = \\mathcal{L}^*(c)$, a single-variable function that parameterizes the input coordinates of the answer(s) to the optimization problem (and also the Lagrange multiplier) with respect to $c$;\n",
    "- doing the multivariable chain rule;\n",
    "- thinking a bit to notice that a lot of things equal zero to get that $\\frac{d\\mathcal{L}^*}{dc} = \\lambda $ ;\n",
    "- and, having remembered that we're interested specifically about the points on $\\mathcal{L}$ that optimize $f$ (which is exactly what $\\mathcal{L}^*(c)$ captures), realizing that the above result implies that first statement of the paragraph before this bulleted list.\n",
    "\n",
    "What a mouthful.\n",
    "\n",
    "\n",
    "### Extension to more than one dimension\n",
    "\n",
    "Would be kind of a shame if we did all of this just to solve problems with just one constraint. But thankfully, the extension is fairly simple to describe!\n",
    "\n",
    "Say we want to optimize $f$ subject to $k$ constraints $g_1 = c_1, g_2 = c_2, \\cdots, g_k = c_k$. Now, in all but the most trivial of cases, it would be impossible to have the gradients of all of these different functions in the same direction. Intuitively (-ish, and assuming you feel comfortable-ish with concepts in linear algebra), if they can't all be in the same direction, you'd think that the \"next best thing\" would be that the gradient of $f$ is in the same direction as some linear combination of the gradients of $g_1, g_2, \\cdots, g_k$, i.e. that\n",
    "\n",
    "$$ \\nabla f = \\sum_{i=1}^k \\lambda _i \\nabla g_i $$\n",
    "\n",
    "That statement above is in fact a condition that is met in the answer to our optimization problem! (How convenient.)\n",
    "\n",
    "Now we have $n$ equations but $n+k$ unknowns. We once again fix that by actually incorporating our constraints:\n",
    "\n",
    "$$ \\nabla f = \\sum_{i=1}^k \\lambda _i \\nabla g_i $$\n",
    "$$ g_1 = c_1 $$\n",
    "$$ ... $$\n",
    "$$ g_k = c_k $$\n",
    "\n",
    "We can once again package everything together as a Lagrangian by having that $\\mathcal{L}_{\\lambda _i} = g_i - c_i$:\n",
    "\n",
    "$$\\begin{align} \\mathcal{L}(x, \\lambda _1, \\cdots, \\lambda _k) &= \n",
    "                                            f - (\\lambda _1 (g_1 - c_1) + \\cdots + \\lambda _k (g_k - c_k) ) \\\\\n",
    "                     &= f - \\sum_{i=1}^k \\lambda _i (g_i - c_i) \\end{align}$$\n",
    "\n",
    "And Bob's your uncle.\n",
    "\n",
    "\\- DK, 4/28/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints with Inequalities: the Karush-Kuhn-Tucker (KKT) Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

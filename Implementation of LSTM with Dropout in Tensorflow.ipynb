{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook was originally being done in order to participate in a Kaggle competition: https://www.kaggle.com/c/msk-redefining-cancer-treatment. Unfortunately, I didn't get to start until two days before the deadline, so I didn't get to make a submission in time, let alone augment the base model to be tailored to the interesting nuances of the data in the competition (same text maps to different classes as multiple genes are studied in one paper; machine-generated fake data confounds the ability to take everything as ground truth with absolute certainty).\n",
    "\n",
    "I enjoyed the incentive to become more familiar with Pandas and Tensorflow. I look forward to studying the Kaggle kernels (e.g. https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm) to see what I may not have known and get a better sense of how else I could have approached the problem.\n",
    "\n",
    "Here's looking forward to the next competition!\n",
    "\n",
    "-DK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = \"training_text_mod_top\" #changed top to allow for easy read using pd.read_csv\n",
    "train_csv = \"training_variants\"\n",
    "glove_fp = \"glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "text_df = pd.read_csv(train_text, sep = r'\\|\\|')\n",
    "labels_df = pd.read_csv(train_csv, sep = ',')\n",
    "# combine to one DataFrame\n",
    "merged_traindev_df = pd.merge(text_df, labels_df, on = 'ID')\n",
    "\n",
    "num_samples = merged_traindev_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize df order, then split into train_df and dev_df\n",
    "shuffled_traindev_df = merged_traindev_df.sample(frac=1)\n",
    "frac_test = 0.8\n",
    "num_test = int(frac_test*num_samples)\n",
    "train_df, dev_df = shuffled_traindev_df.iloc[:num_test], shuffled_traindev_df[num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# presume we have arrays of inputs and targets from, e.g., Pandas DataFrame\n",
    "# X is size [num_features x num_samples]\n",
    "# Y is size [num_samples]\n",
    "\n",
    "# in this case, tokens are our \"features\" and are replaced by 300-D representations\n",
    "# so num_features should be replaced by [representation_dim x num_tokens]\n",
    "#\n",
    "# we'll emulate the varying number of tokens with np.random.choice\n",
    "# and presume we've already done a search-and-replace of \"token\" to \"representation of token\"\n",
    "# in the original input (or initialized unknown words with small random values)\n",
    "#\n",
    "# we'll also pad X with window_radius rows of zeros at the beginnings and ends of the sentences\n",
    "# so that the beginning and end of the paragraph are valid\n",
    "# X = np.random.random(size = (num_samples, representation_dim, np.random.choice(100)))\n",
    "# y = np.random.randint(0, high = num_classes, size = (num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_token2index(df, frac = 0.5, reverse = False):\n",
    "    \"\"\"\n",
    "    Return a dictionary mapping the n most common tokens in the dataframe to an index [0, ..., n-1]\n",
    "    If n is None or greater than the number of distinct tokens, returns a mapping of all words.\n",
    "    If reverse is set to True, returns the n *least* common tokens.\n",
    "    Information about how token2index was created stored in key 'CREATION INFO'.\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    for index, row in df.iterrows():\n",
    "        c.update(word_tokenize(row.loc['Text']))\n",
    "    \n",
    "    if frac < 0 or frac > 1:\n",
    "        num = None\n",
    "    else:\n",
    "        num = int(frac * len(d))\n",
    "    \n",
    "    if reverse:\n",
    "        token2index = {token[0]:i for i,token in enumerate(reversed(c.most_common(num)))}\n",
    "    else:\n",
    "        token2index = {token[0]:i for i,token in enumerate(c.most_common(num))}\n",
    "    \n",
    "    # 'CREATION INFO' will not override any actual tokens as this key contains whitespace\n",
    "    token2index['CREATION INFO'] = ('frac = {0}'.format(frac), 'reverse = {0}'.format(reverse))\n",
    "    \n",
    "    with open('token2index.p', mode = 'wb') as f:\n",
    "        pickle.dump(token2index, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return token2index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_token2index(df):\n",
    "    \"Return a dictionary of all the tokens in df.loc['Text'] to their corresponding frequency ranks (0-indexed).\"\n",
    "    c = Counter()\n",
    "    for index, row in df.iterrows():\n",
    "        c.update(word_tokenize(row.loc['Text']))    \n",
    "    token2index = {token[0]:i for i,token in enumerate(c.most_common())}\n",
    "    \n",
    "    with open('token2index.p', mode = 'wb') as f:\n",
    "        pickle.dump(token2index, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return token2index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_text_counter(df):\n",
    "    \"Return a dictionary of all the tokens in df.loc['Text'] to their corresponding frequency ranks (0-indexed).\"\n",
    "    c = Counter()\n",
    "    for index, row in df.iterrows():\n",
    "        c.update(word_tokenize(row.loc['Text']))    \n",
    "    \n",
    "    with open('text_counter.p', mode = 'wb') as f:\n",
    "        pickle.dump(c, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_counter = make_text_counter(merged_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conveniently, already done this time around\n",
    "with open('text_counter.p', mode = 'rb') as f:\n",
    "    text_counter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, my computer can't handle a ton of ~300k dimensional vectors floating around, even if they're one-hot\n",
    "# (~300k is the actual number of different tokens)\n",
    "# so we'll cut down to near, say, the ~30th percentile and ignore all <unk>'s encountered in the training data\n",
    "lexicon_size = int(0.1*len(text_counter))\n",
    "\n",
    "token2index = {token[0]:i for i, token in enumerate(text_counter.most_common()[int(0.3*len(text_counter) - 0.5 * lexicon_size):\\\n",
    "                                                                             int(0.3*len(text_counter) + 0.5 * lexicon_size)])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon_size = len(token2index)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow section\n",
    "\n",
    "Major thanks to tutorial series starting at\n",
    "https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767\n",
    "\n",
    "(Though I am still slightly confused about a couple things. Oh well, time for empiricism!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.variable_scope.VariableScope at 0x1e75d90de10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "tf.VariableScope(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # neighborhood around which to update token representation (toward context and toward class labels)\n",
    "# # total context goes from [-window_radius + i, window_radius + i] for token at index i\n",
    "# # so total context size is 2*window_radius + 1\n",
    "# window_radius = 3\n",
    "# batch_size = 2*window_radius + 1\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# how many timesteps forward are taken from initial batch location\n",
    "# (will need to ensure we aren't going past the end of the sequence!) (???)\n",
    "truncated_backprop_length = 16\n",
    "\n",
    "consec_token_length = 200\n",
    "representation_dim = 100\n",
    "num_layers = 3\n",
    "state_size = 20\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we convert our words directly into tf.one_hot() Tensors instead of feeding a NumPy array into a tf.placeholder()\n",
    "# (largely because encoding such a NumPy array is either inelegant or memory-intensive)\n",
    "def choose_batch(df, n = batch_size, num_tokens = 50):\n",
    "    '''\n",
    "    Input:\n",
    "    df - DataFrame containing both input text ('Text') and labels ('Class').\n",
    "    n - batch size\n",
    "    Outputs:\n",
    "    X_arr - (batch_size, num_words, lexicon_size) NumPy array of one-hot vectors \n",
    "    corresponding to a string of num_tokens tokens held in token2index and located in the sample.\n",
    "    These tokens are *not* necessary immediately consecutive -- some intervening tokens not in the lexicon\n",
    "    will have been removed.\n",
    "    Y_tensor - (batch_size, num_classes) NumPy array of one-hot vectors corresponding to correct class\n",
    "    '''\n",
    "    # first we sample from all entries at random\n",
    "    sample = df.sample(n)\n",
    "    # now we extract out rows\n",
    "    X, Y = [], []\n",
    "    while len(X) < n:\n",
    "        try:\n",
    "            for index, row in df.sample(n).iterrows():\n",
    "                text, target = row.loc['Text'], row.loc['Class']\n",
    "                index_list = [token2index[token] for token in word_tokenize(text) if token in token2index]\n",
    "        #         pdb.set_trace()\n",
    "                rand_i = random.randint(0, len(index_list) - num_tokens - 1)\n",
    "\n",
    "                tokens_sample = index_list[rand_i:rand_i + num_tokens]\n",
    "                X.append(tf.one_hot(tokens_sample, lexicon_size)), Y.append(tf.one_hot(target, num_classes))\n",
    "                if len(X) == n:\n",
    "                    break\n",
    "        except ValueError: #not enough recognized tokens in a sample\n",
    "            pass # try another sample\n",
    "    # stack along batch axis (chosen to be axis=0) before returning\n",
    "    X_tensor, Y_tensor = tf.stack(X, axis = 0, name = 'batch_X'), tf.stack(Y, axis = 0, name = 'batch_Y')\n",
    "    with tf.Session(): #evaluate into NumPy arrays\n",
    "        X_arr, Y_arr = X_tensor.eval(), Y_tensor.eval()\n",
    "    return X_arr.astype(np.bool), Y_arr.astype(np.bool)\n",
    "#     return X_tensor, Y_tensor\n",
    "        \n",
    "        \n",
    "# test_X, test_Y = choose_batch(train_df, n = 100, num_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def choose_batch(df, n = batch_size):\n",
    "#     '''\n",
    "#     Input:\n",
    "#     df - DataFrame containing both input text ('Text') and labels ('Class').\n",
    "#     n - batch size\n",
    "#     Outputs:\n",
    "#     X - (batch_size, num_words, lexicon_size) tf.Tensor of one-hot vectors corresponding to words in sample\n",
    "#     y - (batch_size, num_classes) tf.Tensor of one-hot vectors corresponding to correct class\n",
    "#     '''\n",
    "#     # first we sample from all entries at random\n",
    "#     sample = df.sample(n)\n",
    "#     # now we extract out rows\n",
    "#     word_vecs = []\n",
    "#     X, Y = [], []\n",
    "#     for index, row in sample.iterrows():\n",
    "#         text, target = row.loc['Text'], row.loc['Class']\n",
    "#         for word in word_tokenize(text):\n",
    "#             word_vecs.append(eye_word[word2index[word]])\n",
    "#         x = np.array(word_vecs)\n",
    "#         X.append(x), Y.append(eye_class[target])\n",
    "#     # return as NumPy arrays for feed dict\n",
    "#     # X is (batch_size, num_words, lexicon_size) and Y is (batch_size, num_classes)\n",
    "#     return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # placeholders\n",
    "\n",
    "# # accepts one-hot representation of sample sentences\n",
    "# # batch_X = tf.placeholder(tf.float32, shape = (batch_size, *(list(X.shape)[1:])), name = 'batch_X')\n",
    "# batch_X = tf.placeholder(tf.float32, shape = (batch_size, None, lexicon_size), name = 'batch_X')\n",
    "\n",
    "# # accepts one-hot representation of correct class\n",
    "# # batch_Y = tf.placeholder(tf.int32, shape = (batch_size, ), name = 'batch_Y')\n",
    "# batch_Y = tf.placeholder(tf.int32, shape = (batch_size, num_classes), name = 'batch_Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_X = tf.placeholder(tf.float32, shape = (batch_size, consec_token_length, lexicon_size), name = 'batch_X')\n",
    "batch_Y = tf.placeholder(tf.int8, shape = (batch_size, num_classes), name = 'batch_Y')\n",
    "# batch_Y = tf.placeholder(tf.bool, shape = (batch_size,), name = 'batch_Y') #for sparse_softmax optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word representation\n",
    "embeddings = tf.Variable(np.random.random(size = (lexicon_size, representation_dim)), \\\n",
    "                         dtype = tf.float32, name = 'word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_mult(a_batch, b, axis = 0):\n",
    "    '''Given a batched rank-(R+1) tf.Tensor a_batch and rank-R tf.Tensor b, return a rank-(R+1) tf.matmul product.'''\n",
    "    a_list = tf.unstack(a_batch, axis = axis)\n",
    "    prod_list = [tf.matmul(a, b) for a in a_list]\n",
    "    return tf.stack(prod_list, axis = axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_embed = tf.matmul(batch_X, embeddings, name = 'batch_embeddings')\n",
    "X_embed = batch_mult(a_batch = batch_X, b = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make \"master\" state, with all layers, both c and h states, for each of the batches, and each of the states\n",
    "lstm_state = tf.Variable(initial_value=tf.zeros((num_layers, 2, batch_size, state_size)))\n",
    "# now divide it down to a list of 2-tuples of (batch_size, state_size) to feed into cell one at a time\n",
    "state_per_layer_list = tf.unstack(lstm_state, axis = 0)\n",
    "# rnn_tuple_state = tuple([state_per_layer_list[i][0], state_per_layer_list[i][1]] for i in range(num_layers))\n",
    "rnn_tuple_state = tuple(tf.nn.rnn_cell.LSTMStateTuple(state_per_layer_list[i][0], state_per_layer_list[i][1]) for i in range(num_layers))\n",
    "\n",
    "\n",
    "# form bidirectional RNN from LSTM cell\n",
    "\n",
    "# fixing \"ValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, but specified shape (20, 40) and found shape (60, 40).\" \n",
    "# as per https://stackoverflow.com/questions/44615147/valueerror-trying-to-share-variable-rnn-multi-rnn-cell-cell-0-basic-lstm-cell-k\n",
    "\n",
    "def lstm_cell():\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size)\n",
    "    # include output dropout as \"ensembling\" method\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "    return cell\n",
    "# stack LSTMs on top of each other\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "cell_outputs, cell_states = tf.nn.dynamic_rnn(cell, X_embed, initial_state = rnn_tuple_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.rand(state_size, num_classes), dtype = tf.float32)\n",
    "b = tf.Variable(np.zeros((1, num_classes)), dtype = tf.float32)\n",
    "\n",
    "# perform final scoring and calculate loss function\n",
    "# we multiply W with the 'h' states of our final LSTM layer\n",
    "logits = tf.add(tf.matmul(cell_states[-1][1], W), b, name = 'logits')\n",
    "# loss function/optimizer\n",
    "loss_fun = tf.losses.softmax_cross_entropy(batch_Y, logits)\n",
    "\n",
    "lr = 0.001 # TODO: schedule learning rate over number of epochs\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # summarizing for Tensorboard\n",
    "\n",
    "# with tf.name_scope('summaries'):\n",
    "#     tf.summary.scalar(loss_fun.name, loss_fun)\n",
    "#     summarizer = tf.summary.merge_all()\n",
    "\n",
    "#     logdir = os.path.join(os.getcwd(), 'logs_train')\n",
    "\n",
    "#     train_writer = tf.summary.FileWriter(logdir, graph = sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at Iteration 0: 2.2990963459014893\n",
      "Testing Loss at Iteration 0: 2.3722589015960693\n",
      "Training Loss at Iteration 1: 2.4208621978759766\n",
      "Testing Loss at Iteration 1: 2.432000160217285\n",
      "Training Loss at Iteration 2: 2.414440631866455\n",
      "Testing Loss at Iteration 2: 2.369863271713257\n",
      "Training Loss at Iteration 3: 2.367913246154785\n",
      "Testing Loss at Iteration 3: 2.3556129932403564\n",
      "Training Loss at Iteration 4: 2.383305549621582\n",
      "Testing Loss at Iteration 4: 2.4195165634155273\n",
      "Training Loss at Iteration 5: 2.3732433319091797\n",
      "Testing Loss at Iteration 5: 2.3215577602386475\n",
      "Training Loss at Iteration 6: 2.3476362228393555\n",
      "Testing Loss at Iteration 6: 2.3816590309143066\n",
      "Training Loss at Iteration 7: 2.3711276054382324\n",
      "Testing Loss at Iteration 7: 2.3617382049560547\n",
      "Training Loss at Iteration 8: 2.369445562362671\n",
      "Testing Loss at Iteration 8: 2.332627773284912\n",
      "Training Loss at Iteration 9: 2.3643910884857178\n",
      "Testing Loss at Iteration 9: 2.219759464263916\n",
      "Training Loss at Iteration 10: 2.4082045555114746\n",
      "Testing Loss at Iteration 10: 2.358222484588623\n",
      "Training Loss at Iteration 11: 2.3697361946105957\n",
      "Testing Loss at Iteration 11: 2.394458055496216\n",
      "Training Loss at Iteration 12: 2.357640266418457\n",
      "Testing Loss at Iteration 12: 2.39070725440979\n",
      "Training Loss at Iteration 13: 2.351402759552002\n",
      "Testing Loss at Iteration 13: 2.366222381591797\n",
      "Training Loss at Iteration 14: 2.3745884895324707\n",
      "Testing Loss at Iteration 14: 2.2956724166870117\n",
      "Training Loss at Iteration 15: 2.2997031211853027\n",
      "Testing Loss at Iteration 15: 2.3737995624542236\n",
      "Training Loss at Iteration 16: 2.3688454627990723\n",
      "Testing Loss at Iteration 16: 2.4473681449890137\n",
      "Training Loss at Iteration 17: 2.358808994293213\n",
      "Testing Loss at Iteration 17: 2.3387012481689453\n",
      "Training Loss at Iteration 18: 2.3668975830078125\n",
      "Testing Loss at Iteration 18: 2.322007179260254\n",
      "Training Loss at Iteration 19: 2.3163187503814697\n",
      "Testing Loss at Iteration 19: 2.3430004119873047\n",
      "Training Loss at Iteration 20: 2.361955165863037\n",
      "Testing Loss at Iteration 20: 2.316561460494995\n",
      "Training Loss at Iteration 21: 2.383108139038086\n",
      "Testing Loss at Iteration 21: 2.2904837131500244\n",
      "Training Loss at Iteration 22: 2.3180289268493652\n",
      "Testing Loss at Iteration 22: 2.298016309738159\n",
      "Training Loss at Iteration 23: 2.321059226989746\n",
      "Testing Loss at Iteration 23: 2.3483848571777344\n",
      "Training Loss at Iteration 24: 2.32318115234375\n",
      "Testing Loss at Iteration 24: 2.3516287803649902\n",
      "Training Loss at Iteration 25: 2.3261771202087402\n",
      "Testing Loss at Iteration 25: 2.3588361740112305\n",
      "Training Loss at Iteration 26: 2.3360185623168945\n",
      "Testing Loss at Iteration 26: 2.315450668334961\n",
      "Training Loss at Iteration 27: 2.3184328079223633\n",
      "Testing Loss at Iteration 27: 2.321237325668335\n",
      "Training Loss at Iteration 28: 2.2854793071746826\n",
      "Testing Loss at Iteration 28: 2.33004093170166\n",
      "Training Loss at Iteration 29: 2.356966495513916\n",
      "Testing Loss at Iteration 29: 2.2993991374969482\n",
      "Training Loss at Iteration 30: 2.2454628944396973\n"
     ]
    }
   ],
   "source": [
    "n_iter = 101 # this one is beefy...\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "# ugly code... clean up...\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        # batch inputs/outputs\n",
    "        X, Y = choose_batch(train_df, n = batch_size, num_tokens = consec_token_length)\n",
    "        feeds = {batch_X: X, batch_Y: Y}\n",
    "        _, loss = sess.run([optimizer, loss_fun], feed_dict = feeds)\n",
    "#         _, loss, summary = sess.run([optimizer, loss_fun, summarizer], feed_dict = feeds)\n",
    "#         train_writer.add_summary(summary, i)\n",
    "        if i % 1 == 0:\n",
    "            print(\"Training Loss at Iteration {0}: {1}\".format(i, loss))\n",
    "            \n",
    "    with tf.name_scope(\"dev\"):\n",
    "        X, Y = choose_batch(dev_df, n = batch_size, num_tokens = consec_token_length)\n",
    "        feeds = {batch_X: X, batch_Y: Y}\n",
    "        loss = sess.run(loss_fun, feed_dict = feeds)\n",
    "#         loss, summary = sess.run([loss_fun, summarizer], feed_dict = feeds)\n",
    "        if i % 1 == 0:\n",
    "            print(\"Testing Loss at Iteration {0}: {1}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

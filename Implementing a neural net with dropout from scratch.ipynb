{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # build \"from scratch\" with numpy\n",
    "from sklearn.datasets import load_boston  # data set to work with\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.datasets import make_moons\n",
    "from functools import wraps\n",
    "from sklearn.utils import shuffle # shuffle two arrays in unison\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress debug logs efficiently (as explained in https://stackoverflow.com/a/2829036/9610637)\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "class DummyFile(object):\n",
    "    def write(self, x):\n",
    "        pass\n",
    "    \n",
    "@contextlib.contextmanager\n",
    "def suppressed_stdout():\n",
    "    real_stdout = sys.stdout\n",
    "    sys.stdout = DummyFile()\n",
    "    yield # everything in 'with' block happens...\n",
    "    sys.stdout = real_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y = True) # a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert y to bins of width 5\n",
    "# # (early part of classification retrofitting)\n",
    "\n",
    "# y_inds = [(val // 5) for val in y]\n",
    "# y_inds = np.array(y_inds, dtype = int)\n",
    "# y_inds -= y_inds.min()\n",
    "# vec_len = y_inds.max() + 1\n",
    "\n",
    "# def convert_to_onehot(y_val, ind_max):\n",
    "#     vec = np.zeros(ind_max, dtype = np.uint8)\n",
    "#     vec[y_val] += 1\n",
    "#     return vec\n",
    "\n",
    "# y_cls = [convert_to_onehot(i, vec_len) for i in y_inds]\n",
    "# y_cls = np.array(y_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_minibatches(X, y, batch_size):\n",
    "    '''\n",
    "    Yield minibatches from X and y of size batch_size.\n",
    "    Copies of X and y are shuffled before yielding batches.\n",
    "    '''\n",
    "    X_shuffled, y_shuffled = shuffle(X, y) # using sklearn.utils.shuffle()\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield X[i:batch_size+i, ...], y[i:batch_size+i, ...]\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wraps\n",
    "def vectorize(f):\n",
    "    '''Make f NumPy-broadcastable.'''\n",
    "    return np.vectorize(f)\n",
    "\n",
    "def perform_dropout(arr, dropped_connection_prop = 0.5):\n",
    "    '''Based on the shape of arr, drop a certain proportion of connections\n",
    "    via a mask, and scale the remaining connections appropriately\n",
    "    (so that the magnitude of output values is similar between training and testing).\n",
    "    Returns the dropped and scaled arr, and the mask.'''\n",
    "    # each connection is present on average, (1 - dropped_connection_prop)\n",
    "    # so when it *is* present, scale it by 1/(1 - dropped_connection_prop)\n",
    "    # then, when *all* connections are being used,\n",
    "    # each connection will be (1 - dropped_connection_prop) its magnitude than during testing,\n",
    "    # but there will be 1/(1 - dropped_connection_prop) as many connections,\n",
    "    # -> 1x the magnitude (without having to do pre-processing to connections)\n",
    "    keep_prob = 1/(1 - dropped_connection_prop)\n",
    "    mask = np.random.random(size=arr.shape) # by default, returns vals in [0, 1)\n",
    "    mask[np.where(mask < keep_prob)] = 1 # which neurons are on\n",
    "    mask[np.where(mask >= keep_prob)] = 0 # turn off other neurons \n",
    "    return arr * mask * keep_prob, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @vectorize # NumPy gets confused by the inequality...\n",
    "def sanitize(v, clip_val = 5):\n",
    "    '''Sanitize values in v (a NumPy array or number) for backprop.'''\n",
    "    try:\n",
    "        for it in np.ndindex(*v.shape):\n",
    "            v[it] = (-2 * (v[it] < 0) + 1) * min(clip_val, abs(v[it]))\n",
    "    except TypeError: # we have a number\n",
    "        v = (-2 * (v < 0) + 1) * min(clip_val, abs(v))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## part of classification retrofitting endeavor\n",
    "# def softmax(arr):\n",
    "#     return np.exp(arr) / np.exp(arr).sum() #np.exp() performs exp() elementwise\n",
    "#\n",
    "# def softmax_loss():\n",
    "#     pass\n",
    "    \n",
    "    \n",
    "    \n",
    "# for least-squares regression\n",
    "@vectorize\n",
    "def loss_function(y_pred, y_actual):\n",
    "    '''Least-squares regression loss function.'''\n",
    "    # we'll use least-squares for this regression model\n",
    "    return 0.5 * (y_pred - y_actual)**2\n",
    "\n",
    "@vectorize\n",
    "def error_signal(y_pred, y_actual):\n",
    "    '''Error signal for least-squares regression.'''\n",
    "    # least-squares error signal\n",
    "    return (y_pred - y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, n_epoch, batch_size = 20, verbose = False):\n",
    "    '''Train the model using X and y a total of n_epoch times.\n",
    "    X and y will be shuffled and fed in minibatches of size batch_size.\n",
    "    \n",
    "    If verbose is set to True, all of the model's internal logging will be written to sys.stdout;\n",
    "    otherwise, only average loss at the end of each epoch is printed out.\n",
    "    '''\n",
    "    # contextlib.ExitStack() is like a 'with no-op', as per\n",
    "    # https://docs.python.org/3/library/contextlib.html#simplifying-support-for-single-optional-context-managers\n",
    "    verbosity_context = contextlib.ExitStack if verbose else suppressed_stdout\n",
    "    \n",
    "    for i_epoch in range(n_epoch):\n",
    "        i_batch, loss = 0, 0\n",
    "        with verbosity_context():\n",
    "            for X_batch, y_batch in yield_minibatches(X, y, batch_size = 20):\n",
    "                i_batch += 1\n",
    "                y_preds = model.forward(X_batch)\n",
    "                loss += loss_function(y_preds, y_batch).mean()\n",
    "                model.backward(y_batch)\n",
    "        loss /= i_batch\n",
    "        print(\"Epoch #{0}. Average loss: {1}\".format(i_epoch+1, loss))\n",
    "    return\n",
    "\n",
    "def test(model, X, y):\n",
    "    model.set_mode('test')\n",
    "    y_preds = model.forward(X)\n",
    "    loss = loss_function(y_preds, y).mean()\n",
    "    print(\"Average error: {0}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, n_features, n_outputs, n_layers=2, n_neurons_per_layer=25):\n",
    "        '''\n",
    "        n_layers: number of *hidden* layers (ie not including output layer).\n",
    "        Using ReLU nonlinearity between layers.\n",
    "        '''\n",
    "        self.lr = 1E-4\n",
    "        self.mode = 'train' # know whether to e.g. implement dropout\n",
    "        self.n_layers = n_layers\n",
    "        self.n_outputs = n_outputs\n",
    "        self.layer_list = ['in', *range(self.n_layers), 'out']\n",
    "            # contain per-run parameters, e.g. hidden layer vals, dropout masks\n",
    "        self.history_dict = defaultdict(dict)\n",
    "            # 'h_in' == X, 'h_out' == y_preds\n",
    "        # initialize layers\n",
    "        self.layers = {}\n",
    "        self.layers['W_in'] = np.random.randn(n_features, n_neurons_per_layer)\n",
    "        self.layers['b_in'] = np.ones(n_neurons_per_layer)\n",
    "        for i in range(n_layers):\n",
    "            self.layers['W_{0}'.format(i)] = np.random.randn(n_neurons_per_layer, n_neurons_per_layer)\n",
    "            self.layers['b_{0}'.format(i)] = np.ones(n_neurons_per_layer)\n",
    "                # start with relatively high bias to avoid having too many immediately dead neurons\n",
    "        self.layers['W_out'] = np.random.randn(n_neurons_per_layer, n_outputs)\n",
    "        self.layers['b_out'] = np.ones(n_outputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Provide a prediction based on input array X. (Assuming order (n_samples, n_features))\n",
    "        '''\n",
    "        cur = X\n",
    "        \n",
    "        for i,x in enumerate(self.layer_list): # all layers\n",
    "            self.history_dict['hs']['h_{}'.format(x)] = cur.copy() # remember inputs for backprop\n",
    "            print(\"Forward prediction at layer '{0}'\".format(x))\n",
    "            cur_W, cur_b = self._prepare_layers(x)\n",
    "#             cur = cur @ cur_W + cur_b # update inputs\n",
    "            # With current backward implementation, biases seem to kill weights\n",
    "            # and turn all output into a small constant.\n",
    "            #\n",
    "            # To fix, would probably need to implement regularization into loss function/error signal\n",
    "            # to prevent gradual creep of biases/death of connections.\n",
    "            # At the same time, would have to balance the regularization penalty\n",
    "            # with the least-squares loss.\n",
    "            # Or, a bit more hacky but could potentially work:\n",
    "            # we could train the weights first (ensuring they don't die immediately),\n",
    "            # then train the biases, and alternate until convergence (on ideally not just a number,\n",
    "            # since the weights will already have some signal by the time the biases are being trained\n",
    "            # and so the output would ideally not just be a constant)\n",
    "            #\n",
    "            # It seems we're not alone in finding regression models temperamental,\n",
    "            # as in the suggestion to use bins here: http://cs231n.github.io/neural-networks-2/\n",
    "            #\n",
    "            # Given the data, it would be reasonable to split y into 10 bins of width 5\n",
    "            # and make a classifier using e.g. softmax\n",
    "            # However, since our goal was simply to implement a neural net from scratch\n",
    "            # and ensure that it functions as intended,\n",
    "            # and we seem to have done so (while getting some practical understanding of\n",
    "            # the difficulties of regression models),\n",
    "            # we shall just dummy out the bias and stick to regression with just weights\n",
    "            # (even though the predicted values are more than a bit dubious)\n",
    "            # rather than retrofit everything for classification\n",
    "            # or impose L2 regularlization\n",
    "            # (Next time!)\n",
    "            cur = cur @ cur_W # dummy out biases\n",
    "\n",
    "\n",
    "            if x != 'out':\n",
    "                cur[np.where(cur < 0)] = 0 # apply ReLU\n",
    "#             else:\n",
    "#                 if self.n_outputs == 1: # regression\n",
    "#                     cur = cur @ cur_W + cur_b\n",
    "#                     break\n",
    "#                 else: # classification. Use softmax (will do without biases)\n",
    "#                     cur = softmax(cur @ cur_W)\n",
    "            print('cur.shape after multiplication: {}'.format(cur.shape))\n",
    "                # dim is always (n_samples, ...)\n",
    "                # dim of cur_b matches last dimension of (cur @ cur_W) \n",
    "                # so bias is broadcasted to each sample (cur's first dim) \n",
    "\n",
    "        self.history_dict['y'] = cur.copy()\n",
    "        return cur\n",
    "            \n",
    "    \n",
    "    def backward(self, y):\n",
    "        '''\n",
    "        Based on results of most recent call to self.forward()\n",
    "        and the correct answers y, update parameters.\n",
    "        \n",
    "        Error signal and loss function values are determined \n",
    "        by calls to loss_function() and error_signal()\n",
    "        '''\n",
    "        y_preds = self.history_dict['y']\n",
    "        errs = error_signal(y_preds, y.reshape(y_preds.shape))\n",
    "        print('errs.shape = {}'.format(errs.shape))\n",
    "        \n",
    "        errs = sanitize(errs)\n",
    "        backprop_list = list(reversed(self.layer_list))\n",
    "            # 'h_in' == X, 'h_out' == Y\n",
    "\n",
    "        for x in backprop_list:\n",
    "            print(\"Backprop at layer '{0}'\".format(x))\n",
    "            W_cur, b_cur = self.layers['W_{0}'.format(x)], self.layers['b_{0}'.format(x)]\n",
    "\n",
    "            mask, h_cur = self.history_dict['masks']['W_{}'.format(x)], \\\n",
    "                          self.history_dict['hs']['h_{}'.format(x)]\n",
    "            print('W_cur.shape = {}, b_cur.shape = {}, mask.shape = {}, h_cur.shape = {}'.\n",
    "                          format(W_cur.shape, b_cur.shape, mask.shape, h_cur.shape))\n",
    "                # update biases\n",
    "            errs = errs.mean(axis = 0, keepdims = True) # average along samples\n",
    "            b_cur -= sanitize(self.lr * errs.squeeze(), clip_val = 1)\n",
    "                # errs.squeeze() <- make dims match (for regression)\n",
    "            self.layers['b_{0}'.format(x)] = b_cur\n",
    "                # update weights\n",
    "            h_cur = h_cur.mean(axis = 0, keepdims = True) # average along samples\n",
    "            # get error per weight parameter\n",
    "            # to figure out order of matmul, take, for example, dims of first backprop step\n",
    "            #\n",
    "            # dim(errs) == (num_samples, n_outputs), dim(h_cur) = (n_samples, n_neurs)\n",
    "            # want to be of shape dim(W_cur) == (n_neurs, n_outputs)\n",
    "            errs = h_cur.T @ errs\n",
    "            errs = sanitize(self.lr * errs, clip_val = 5)\n",
    "            W_cur -= errs * mask\n",
    "            self.layers['W_{0}'.format(x)] = W_cur\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def _prepare_layers(self, key_id):\n",
    "        '''Prepare weights/biases for multiplication (i.e., perform dropout or not).\n",
    "        Indicate which layer using key_id, a string.\n",
    "        \n",
    "        Returns the prepared layers as a tuple in order (W,b)\n",
    "        and adds relevant info to self.history_dict, if applicable.\n",
    "        '''\n",
    "        W, b = self.layers['W_{}'.format(key_id)], self.layers['b_{}'.format(key_id)]\n",
    "        if self.mode == 'train': # perform dropout of connections\n",
    "            W, mask = perform_dropout(W, dropped_connection_prop = 0.5)\n",
    "            # remember mask for proper backprop\n",
    "            print('After perform_dropout, W.shape = {}, mask.shape = {}'.format(W.shape, mask.shape))\n",
    "            self.history_dict['masks']['W_{}'.format(key_id)] = mask\n",
    "        else:\n",
    "            # be compatible with self.backward()\n",
    "            self.history_dict['masks']['W_{}'.format(key_id)] = np.ones(W.shape, dtype = np.bool)\n",
    "        return W,b\n",
    "            \n",
    "    \n",
    "    def set_mode(self, mode_flag):\n",
    "        \"\"\"Switch between training mode ('train') and testing mode ('test').\"\"\"\n",
    "        self.mode = mode_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_features = X_train.shape[-1], n_outputs = 1) # y is 1-D so can't do y.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(model, X_train, y_train, n_epoch = 100, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
